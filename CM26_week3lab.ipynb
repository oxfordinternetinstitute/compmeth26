{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e965fb5",
   "metadata": {},
   "source": [
    "# Computational Methods - HT26 \n",
    "## Oxford Internet Institute\n",
    "## Week 3 - Parsing online data\n",
    "\n",
    "Note: This formative will be marked by the TA and/or the instructor. It is due Monday of Week 5 at 11am.\n",
    "\n",
    "**Data**: Reddit - collect yourself; only SFW subs that do not require registration.\n",
    "\n",
    "**Claim**: To be determined by you. \n",
    "\n",
    "**Representation**: Comparison between two groups. This can be statistical or narrative. You are welcome to use graphics. \n",
    "\n",
    "**AI tooling**: I believe that AI code is likely to be _more_ robust and accurate than self-written code. You are welcome to use AI for this work. Much of the key conceptual work will be involved in appropriate case selection for a legitimate comparison between two subreddits. \n",
    "\n",
    "- You will see that there is a lot of code below. It is mostly AI written but it does follow my logic very closely. For this assignment, I will assume that you will be using AI but I will also assume that you will be providing: \n",
    "  - **Skepticism**. How can you establish when your code wrong, when the data is off? \n",
    "  - **Keen eye for defaults**. There are a couple places where I created a little break. Notice below where I wrote `if i > 5: break`. That was a way to break after the first 5 stories. It should only be used in testing. You'll want all 25 stories for your work. \n",
    "  - **Focus on your core tasks and goals**. AI can sometimes be a bit enthusiastic. It might recommend tests or approaches that you do not fully understand. You must ensure that all claims are defensible within your write-up. AI should NOT be used for any writing, only for the code that follows your prompting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936900e",
   "metadata": {},
   "source": [
    "# Exercise 1. Getting the data from online\n",
    "\n",
    "We will be downloading Reddit data \"the hard way\". This is also the slow way, but it is useful for this exercise. You will need the `requests` library, but that should be available in Python already.\n",
    "\n",
    "First, we will want to create a means of collecting a single Reddit post. Navigate online to find a post of your choice and then notice the first part of the URL. For example, if I go to: \n",
    "https://www.reddit.com/r/LabourUK/comments/1qtp6ee/in_gorton_and_denton_i_found_a_longfestering/\n",
    "\n",
    "Then replace www.reddit.com with api.reddit.com, the data will download in JSON. We will collect this data. \n",
    "\n",
    "Below, simply replace the <...> with a URL to collect the JSON. However, you will also need to edit the 'header' of the file so that you are uniquely identified. Then I want you to navigate this file. What we are looking to do is find the part where there is a list of comments. Report how many comments came down and compare this to the number of comments reported on the site. \n",
    "\n",
    "We want to know how many different users made a comment in this specific data. This is not the 'full comment tree' but instead is a truncated tree where you can request more. We will not be doing that in this lab. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4eb92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070dbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_reddit_post(url, user_agent):\n",
    "    \"\"\"\n",
    "    Download Reddit post data as JSON.\n",
    "    \n",
    "    Args:\n",
    "        url: A reddit post URL (www.reddit.com or api.reddit.com)\n",
    "        user_agent: Unique identifier for your requests (edit this!)\n",
    "    \n",
    "    Returns:\n",
    "        Parsed JSON data from the Reddit API\n",
    "    \"\"\"\n",
    "    # Convert www.reddit.com to api.reddit.com if needed\n",
    "    api_url = url.replace(\"www.reddit.com\", \"api.reddit.com\")\n",
    "    \n",
    "    # Ensure URL ends with .json (alternative to using api subdomain)\n",
    "    if not api_url.endswith(\".json\"):\n",
    "        api_url = api_url.rstrip(\"/\") + \".json\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": user_agent\n",
    "    }\n",
    "    \n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    response.raise_for_status()  # Raise exception for HTTP errors\n",
    "    \n",
    "    data = response.json()  # Equivalent to json.loads(response.text)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_reddit_data(data):\n",
    "    \"\"\"\n",
    "    Parse Reddit JSON data into DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        data: JSON data returned from get_reddit_post()\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (post_df, comments_df)\n",
    "    \"\"\"\n",
    "    # The data is a list with 2 elements:\n",
    "    # data[0] contains the post\n",
    "    # data[1] contains the comments\n",
    "    \n",
    "    # Parse the post (it's nested under data -> children -> [0] -> data)\n",
    "    post_df = json_normalize(data[0][\"data\"][\"children\"])\n",
    "    \n",
    "    # Parse the comments (also under data -> children)\n",
    "    comments_df = json_normalize(data[1][\"data\"][\"children\"])\n",
    "    \n",
    "    return post_df, comments_df\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"\n",
    "    Remove 'data.' prefix from DataFrame column names.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'data.' prefixed column names\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with cleaned column names\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.str.replace(\"data.\", \"\", regex=False)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    post_url = \"https://www.reddit.com/r/LabourUK/comments/1qtp6ee/in_gorton_and_denton_i_found_a_longfestering/\"\n",
    "    my_user_agent = \"CompMethods Lab Exercise (u/<...>)\n",
    "    \n",
    "    data = get_reddit_post(post_url, user_agent=my_user_agent)\n",
    "    \n",
    "    post_df, comments_df = parse_reddit_data(data)\n",
    "\n",
    "    posts_df = clean_column_names(post_df)\n",
    "    \n",
    "    # Check if the post is stickied\n",
    "    is_stickied = post_df[\"stickied\"].iloc[0]\n",
    "    print(f\"Is post stickied? {is_stickied}\")\n",
    "    \n",
    "    comments_df = clean_column_names(comments_df)\n",
    "    \n",
    "    print(\"Post DataFrame:\")\n",
    "    print(f\"  Shape: {post_df.shape}\")\n",
    "    print(f\"  Columns: {list(post_df.columns)[:10]}...\")  # First 10 columns\n",
    "    \n",
    "    print(\"\\nComments DataFrame:\")\n",
    "    print(f\"  Shape: {comments_df.shape}\")\n",
    "    print(f\"  Columns: {list(comments_df.columns)[:10]}...\")\n",
    "    \n",
    "    # Preview the comments\n",
    "    if \"data.author\" in comments_df.columns:\n",
    "        print(f\"\\nNumber of comments downloaded: {len(comments_df)}\")\n",
    "        print(f\"Unique authors: {comments_df['data.author'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fafa0c",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2\n",
    "\n",
    "Building this out: Now let's navigate to a subreddit and do the same thing. So for example, make a request to http://api.reddit.com/r/aww . However, I want you to request the top stories this year. This involves using an \"argument\". For \"aww\" the example would be: \n",
    "\n",
    "> https://api.reddit.com/r/aww/top/?t=year\n",
    "\n",
    "This time we are looking for the posts. Each story is a single item in the json. \n",
    "\n",
    "Report how many stories were collected. The default is 25 but there may be more if there is a post which is \"stickied\". Identify the stickied post and remove it. For the remaining posts, collect the comments under the story.\n",
    "\n",
    "We will want to create two DataFrames - one for posts and one for comments on the stories for each post. Notice that we do not store all of the data here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc370c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching top posts from r/news...\n",
      "  Fetching post 1/25: Luigi Mangione accepts nearly $300K in d...\n",
      "  Fetching post 2/25: Elon Musk and Prince Andrew named in lat...\n",
      "  Fetching post 3/25: Personal information of 4,500 ICE and Bo...\n",
      "  Fetching post 4/25: ABC suspends Jimmy Kimmel’s late-night s...\n",
      "  Fetching post 5/25: Tesla investor calls for Elon Musk to st...\n",
      "  Fetching post 6/25: As top Trump aides sent texts on Signal,...\n",
      "Posts collected: 6\n",
      "Comments collected: 219\n",
      "Unique commenters: 192\n",
      "\n",
      "Comments per post:\n",
      "count     6.000000\n",
      "mean     36.500000\n",
      "std      13.560973\n",
      "min      18.000000\n",
      "25%      27.500000\n",
      "50%      38.000000\n",
      "75%      44.750000\n",
      "max      54.000000\n",
      "dtype: float64\n",
      "\n",
      "Sample comments:\n",
      "                 author                                               body  \\\n",
      "0      TalmadgeReyn0lds  My MS medication costs $163k every 6 months, w...   \n",
      "1        brickyardjimmy    Please, oh please, let this trial be televised.   \n",
      "2          Insciuspetra         So..\\n\\nOne week in a hospital beds worth.   \n",
      "3  Historical_Avocado_8  **VERY IMPORTANT REMINDER:**\\n\\nIf you’re a po...   \n",
      "4         brokenmessiah  All the love he was getting in surprised not i...   \n",
      "\n",
      "     ups  \n",
      "0   3921  \n",
      "1  14980  \n",
      "2  16893  \n",
      "3   2997  \n",
      "4   3294  \n"
     ]
    }
   ],
   "source": [
    "def get_subreddit(subreddit, user_agent, time_filter=\"year\", rating_filter=\"top\"):\n",
    "    \"\"\"\n",
    "    Download top posts from a subreddit (stage 1: get post list).\n",
    "    \"\"\"\n",
    "    url = f\"https://api.reddit.com/r/{subreddit}/{rating_filter}/?t={time_filter}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": user_agent\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_post_details(permalink, user_agent):\n",
    "    \"\"\"\n",
    "    Download full details for a single post including comments.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.reddit.com{permalink}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": user_agent\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def build_dataframes(subreddit, user_agent, time_filter=\"year\", rating_filter=\"top\"):\n",
    "    \"\"\"\n",
    "    Two-stage download: get post list, then fetch each post's comments.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (posts_df, comments_df)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Stage 1: Get list of top posts\n",
    "    print(f\"Fetching top posts from r/{subreddit}...\")\n",
    "    listing_data = get_subreddit(subreddit, time_filter, user_agent, rating_filter)\n",
    "    \n",
    "    posts = []\n",
    "    comments = []\n",
    "    children = listing_data[\"data\"][\"children\"]\n",
    "    \n",
    "    # Stage 2: Fetch each post's full details and comments\n",
    "    for i, child in enumerate(children):\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if i > 5: break\n",
    "        \n",
    "        post_summary = child[\"data\"]\n",
    "        permalink = post_summary[\"permalink\"]\n",
    "        \n",
    "        print(f\"  Fetching post {i+1}/{len(children)}: {post_summary['title'][:40]}...\")\n",
    "        \n",
    "        # Get full post data (index 0 = post, index 1 = comments)\n",
    "        post_data = get_post_details(permalink, user_agent)\n",
    "        post_full = post_data[0][\"data\"][\"children\"][0][\"data\"]\n",
    "        post_id = post_full[\"id\"]\n",
    "        \n",
    "        # Extract post info\n",
    "        posts.append({\n",
    "            \"id\": post_id,\n",
    "            \"subreddit\": subreddit,\n",
    "            \"author\": post_full[\"author\"],\n",
    "            \"created_utc\": post_full[\"created_utc\"],\n",
    "            \"title\": post_full[\"title\"],\n",
    "            \"url\": post_full[\"url\"],\n",
    "            \"ups\": post_full[\"ups\"],\n",
    "            \"num_comments\": post_full[\"num_comments\"],\n",
    "            \"stickied\": post_full[\"stickied\"],\n",
    "        })\n",
    "        \n",
    "        # Extract comments\n",
    "        comment_children = post_data[1][\"data\"][\"children\"]\n",
    "        for comment_child in comment_children:\n",
    "            # Skip \"more\" objects (truncated comment lists)\n",
    "            if comment_child[\"kind\"] != \"t1\":\n",
    "                continue\n",
    "            \n",
    "            comment_data = comment_child[\"data\"]\n",
    "            comments.append({\n",
    "                \"comment_id\": comment_data[\"id\"],\n",
    "                \"post_id\": post_id,\n",
    "                \"subreddit\": subreddit,\n",
    "                \"author\": comment_data[\"author\"],\n",
    "                \"body\": comment_data[\"body\"],\n",
    "                \"created_utc\": comment_data[\"created_utc\"],\n",
    "                \"ups\": comment_data[\"ups\"],\n",
    "                \"score\": comment_data[\"score\"],\n",
    "            })\n",
    "        \n",
    "        # Be polite to Reddit's servers\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    posts_df = pd.DataFrame(posts)\n",
    "    comments_df = pd.DataFrame(comments)\n",
    "    \n",
    "    return posts_df, comments_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    \n",
    "    # my_user_agent = \"CompMethods Lab Exercise (u/berniehogan)\"\n",
    "    my_user_agent = \"CompMethods Lab Exercise (u/<...>) # edit\n",
    "    \n",
    "    posts_df, comments_df = build_dataframes(\"news\", time_filter=\"year\", user_agent=my_user_agent)\n",
    "    \n",
    "    # Remove stickied posts\n",
    "    stickied_ids = posts_df[posts_df[\"stickied\"] == True][\"id\"].tolist()\n",
    "    posts_df = posts_df[posts_df[\"stickied\"] == False]\n",
    "    comments_df = comments_df[~comments_df[\"post_id\"].isin(stickied_ids)]\n",
    "    \n",
    "    print(f\"Posts collected: {len(posts_df)}\")\n",
    "    print(f\"Comments collected: {len(comments_df)}\")\n",
    "    print(f\"Unique commenters: {comments_df['author'].nunique()}\")\n",
    "    \n",
    "    print(\"\\nComments per post:\")\n",
    "    print(comments_df.groupby(\"post_id\").size().describe())\n",
    "    \n",
    "    print(\"\\nSample comments:\")\n",
    "    print(comments_df[[\"author\", \"body\", \"ups\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c552f",
   "metadata": {},
   "source": [
    "# Part 3. Comparing two subs\n",
    "\n",
    "Now you have the means to create a DataFrame for a sub, and a DataFrame of comments. Now here's where your work really begins: \n",
    "\n",
    "- Compare two subreddits using a research question that can be answered with the columns we have selected. \n",
    "- This is wide open for you. Some ideas:\n",
    "  - Compare two subs with a regional focus. How often does a keyword appear in the title of the two subs.\n",
    "  - Compare two political subs and report on the different news sites that they used. \n",
    "  - If you want to use time data: \n",
    "    -  Consider the top posts of the last year: Were they in different times of the year? \n",
    "    - Do the top posts get posted a specific time of day?\n",
    "  - Compare the posters in the comments: \n",
    "    - How many people are featured in multiple posts? Do people who post in multiple posts tend to get more upvotes on average? \n",
    "\n",
    "What we will be looking for: \n",
    "- **Code**: How clean is it? Do you use methods / functions? \n",
    "- **Comparison**: Is it a fair comparison or a trivial one? \n",
    "- **Write up**: How do you interpret your results? Have you used a table or an image to display some work? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0afc3c",
   "metadata": {},
   "source": [
    "# Your answer: \n",
    "\n",
    "In your answer first produce all the code cells then produce a short write up of less than 500 words. You will want to consider:\n",
    "- Am I going to create two DFs, one for each subreddit or am I going to merge them into a single DF? \n",
    "- What statistics will I use to compare if any? \n",
    "- What should we interpret from these results? \n",
    "\n",
    "Below your answer write your own AI declaration. Within this, be sure to remark on any times where the AI led you astray, did things you couldn't explain or otherwise did not work as predicted. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2dc5dd",
   "metadata": {},
   "source": [
    "# AI Declaration \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820cbd0",
   "metadata": {},
   "source": [
    "The code was written primarily by Claude. I wrote the summaries and fed it, read the code, ran it and tweaked it. I ensured that we have a means for user-agent strings and `time.sleep()`. \n",
    "\n",
    "Neither Claude nor myself had luck in downloading selftext (the body of the post).\n",
    "\n",
    "For students, you are welcome to use an AI to assist you in this task. The important aspects of this task will be down to:\n",
    "- Checking that the results match what you expect. \n",
    "- That your research question is grounded and coherent. Ideally with some reference to prior literature where possible.\n",
    "- Checking that you will use a statistical test that is relevant for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bc208",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
